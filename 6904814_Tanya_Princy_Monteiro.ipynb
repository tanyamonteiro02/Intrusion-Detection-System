{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1wcnjQODYQ4"
      },
      "source": [
        "# ***0. Data Loading***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "n6MJyx0bDlFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c5e4c4-9f98-4162-c2c1-bfd38677ac48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4000, 44)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "#import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(68)\n",
        "\n",
        "#load the training and test1 data\n",
        "traindata = pd.read_csv('UNSWNB15_training_coursework.csv')\n",
        "testdata = pd.read_csv('UNSWNB15_testing1_coursework.csv')\n",
        "\n",
        "#display the rows and columns\n",
        "traindata.head()\n",
        "testdata.head()\n",
        "\n",
        "#check the shape of data\n",
        "traindata.shape\n",
        "testdata.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYce1Z49mIsz"
      },
      "source": [
        "# ***1. Data Pre-Processing (Task 1)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "A0DbTy5EA0f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "234c96bd-d093-40f2-ce82-e8912b076e1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['proto', 'service', 'state'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "\"\"\"encode categorical variables, align columns, and standardize numerical features using z-score normalization.\n",
        "to ensure numerical stability and model convergence.\"\"\"\n",
        "\n",
        "#identify categorical data and perform one hot encoding\n",
        "categorical_data= traindata.select_dtypes(include=['object']).columns\n",
        "print(categorical_data)\n",
        "traindata = pd.get_dummies(traindata, columns=categorical_data)\n",
        "testdata = pd.get_dummies(testdata, columns=categorical_data)\n",
        "\n",
        "\n",
        "#align both the dataset to ensure both datasets have same feature columns\n",
        "traindata, testdata = traindata.align(testdata, join='left', axis=1,fill_value=0)\n",
        "\n",
        "# seperate the features and labels\n",
        "trainx= traindata.drop(columns=['label']).values #features\n",
        "trainy= traindata['label'].values.reshape(-1,1)  #label\n",
        "testx1= testdata.drop(columns=['label']).values\n",
        "testy1 = testdata['label'].values.reshape(-1,1)\n",
        "\n",
        "#convert the data to numeric float\n",
        "trainx = np.asarray(trainx, dtype=np.float64)\n",
        "testx1 = np.asarray(testx1, dtype=np.float64)\n",
        "\n",
        "#standardization\n",
        "mean_avg= np.mean(trainx,axis=0)\n",
        "st_dev= np.std(trainx,axis=0)\n",
        "trainx= (trainx-mean_avg)/st_dev\n",
        "testx1= (testx1-mean_avg)/st_dev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4uQmeM_Albh"
      },
      "source": [
        "# ***2. Model Implementation and Training (Task 2)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "yzR1gaj0BZlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d316f5b-0b1f-40ec-f004-2431d4321e7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:100 loss:0.8284 accuracy:0.8865\n",
            "epoch:200 loss:0.8219 accuracy:0.8921\n",
            "epoch:300 loss:0.1906 accuracy:0.9127\n",
            "epoch:400 loss:0.1935 accuracy:0.9166\n",
            "epoch:500 loss:0.2079 accuracy:0.9150\n",
            "epoch:600 loss:0.1549 accuracy:0.9395\n",
            "epoch:700 loss:0.1451 accuracy:0.9482\n",
            "epoch:800 loss:0.1592 accuracy:0.9353\n",
            "epoch:900 loss:0.3029 accuracy:0.8908\n",
            "epoch:1000 loss:0.1528 accuracy:0.9402\n"
          ]
        }
      ],
      "source": [
        "\"\"\"implementing a Multi-Layer Perceptron (MLP) from scratch using NumPy,\n",
        "including forward & backward propagation with binary class entropy using batch gradient descent.\n",
        "after network is trained on the processed data.\"\"\"\n",
        "\n",
        "#activation functions:(relu for the hidden layer and sigmoid for  the output layer)\n",
        "#relu function\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "#relu derivative\n",
        "def relu_deri(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "#sigmoid function\n",
        "def sigmoid(x):\n",
        "    return np.where(x >= 0,\n",
        "                    1/(1 + np.exp(-x)),\n",
        "                    np.exp(x)/(1 + np.exp(x)))\n",
        "#sigmoid derivative\n",
        "def sigmoid_deri(x):\n",
        "    sig = sigmoid(x)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "#loss function ( binary cross-entropy is used to measure the difference between predicted and actual labels for binary classification)\n",
        "def binarycross_entropy_(y_true, y_pred):\n",
        "     y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "     return -(y_true * np.log(y_pred)+(1 - y_true) * np.log(1 - y_pred)).mean()\n",
        "\n",
        "#mlp model\n",
        "class MLP:\n",
        "    def __init__(self, input, hiddenlayer1, hiddenlayer2, output):\n",
        "        #initialization weights and bias for layer1 using He initialization which helps maintain a stable variance of activations and gradients throughout the network, preventing exploding or vanishing gradients.\n",
        "        self.w_hidden1 = np.random.randn(input,hiddenlayer1) * np.sqrt(2/input)\n",
        "        self.b_h1= np.zeros((1,hiddenlayer1))\n",
        "\n",
        "        #initialization weights and bias for layer2\n",
        "        self.w_hidden2 = np.random.randn(hiddenlayer1,hiddenlayer2) * np.sqrt(2/hiddenlayer1)\n",
        "        self.b_h2 = np.zeros((1,hiddenlayer2))\n",
        "\n",
        "        #initialization weights and bias for output\n",
        "        self.w_output = np.random.randn(hiddenlayer2,output) * np.sqrt(2/hiddenlayer2)\n",
        "        self.b_output =np.zeros((1,output))\n",
        "\n",
        "    #forward pass(predict output)\n",
        "    def forward(self, x):\n",
        "        self.h_input1 = np.dot(x,self.w_hidden1) + self.b_h1\n",
        "        self.h_output1 = relu(self.h_input1)\n",
        "\n",
        "        self.h_input2 = np.dot(self.h_output1,self.w_hidden2) + self.b_h2\n",
        "        self.h_output2 = relu(self.h_input2)\n",
        "\n",
        "        # from inputs, weights and biases calculate the output values\n",
        "        self.finalinput = np.dot(self.h_output2,self.w_output) + self.b_output\n",
        "        self.finaloutput = sigmoid(self.finalinput)\n",
        "\n",
        "        return self.finaloutput\n",
        "\n",
        "    #backward pass (calculate how much each neuron contributed to the total error.)\n",
        "    def backward(self, x, y_true, learningrate=0.001):\n",
        "        #compute output layer error and delta\n",
        "        error = self.finaloutput - y_true\n",
        "        delta = error *  sigmoid_deri(self.finalinput)\n",
        "\n",
        "        #backpropagate through hidden layer 2\n",
        "        error2 =  delta.dot(self.w_output.T)\n",
        "        delta2 =  error2 * relu_deri(self.h_input2)\n",
        "\n",
        "        #backpropagate through hidden layer 1\n",
        "        error1 = delta2.dot(self.w_hidden2.T)\n",
        "        delta1 = error1 * relu_deri(self.h_input1)\n",
        "\n",
        "        #-----update weights and biases using gradient descent----\n",
        "\n",
        "        #update weight and bias for output layer\n",
        "        self.w_output -= learningrate * self.h_output2.T.dot(delta)\n",
        "        self.b_output -=  learningrate * np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "        #update weight and bias for hidden layer 2\n",
        "        self.w_hidden2 -=  learningrate * self.h_output1.T.dot(delta2)\n",
        "        self.b_h2 -= learningrate * np.sum(delta2, axis=0, keepdims=True)\n",
        "\n",
        "        #update weight and bias for hidden layer 1\n",
        "        self.w_hidden1 -= learningrate * x.T.dot(delta1)\n",
        "        self.b_h1 -= learningrate * np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "    #train the MLP model\n",
        "    def train(self,x,y,epochs=300,learningrate=0.001):\n",
        "        for epoch in range(epochs):\n",
        "            ypred = self.forward(x)                    # Forward pass (y_pred = predicted output)\n",
        "            loss = binarycross_entropy_(y, ypred)      # Compute loss (y = label data)\n",
        "            self.backward(x, y, learningrate)          # Backward pass (update weights)\n",
        "\n",
        "            # for every 100 epochs print loss and accuracy\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                predictions = (ypred > 0.5).astype(int)\n",
        "                accuracy = (predictions == y).mean()\n",
        "                print(f'epoch:{epoch+1} loss:{loss:.4f} accuracy:{accuracy:.4f}')\n",
        "\n",
        "\n",
        "#--------model implementation and training------------\n",
        "\n",
        "#initialize MLP architecture: input -> 64 -> 32 -> 1\n",
        "mlp = MLP(input=trainx.shape[1],hiddenlayer1=64,hiddenlayer2=32,output=1)\n",
        "\n",
        "#training the mlp model on trianing data\n",
        "mlp.train(trainx,trainy,epochs=1000,learningrate=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIM6lyE-BA1B"
      },
      "source": [
        "# ***3. Model Performance Evaluation (Task 3)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "filj8Tb2Bav9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e1c8cd-7386-493d-e889-b9741c51a7c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Testing Set 1: 0.9050\n",
            "\n",
            " Confusion matrix:\n",
            " TP: 2373, FP: 61\n",
            " FN: 319, TN: 1247\n",
            "\n",
            " Accuracy:0.9050\n",
            " Balanced Accuracy:0.9174\n",
            "\n",
            " Recall:0.8815\n",
            " Precision:0.9749\n",
            " f1 Score:0.9259\n",
            "\n",
            "Predictions for Test Set 2:\n",
            "[0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1]\n"
          ]
        }
      ],
      "source": [
        "\"\"\" model performance evaluation on test set1\n",
        "using confusion matrix,accuracy, balanced accuracy,recall,precision and F1-score\n",
        "and also predicting labels for test set2.\"\"\"\n",
        "\n",
        "test1_predictions =(mlp.forward(testx1) > 0.5).astype(int)\n",
        "test1_accuracy =(test1_predictions == testy1).mean()\n",
        "print(f\"Accuracy on Testing Set 1: {test1_accuracy:.4f}\\n\")\n",
        "\n",
        "def evaluation(ytrue_, ypred_):\n",
        "    TrueP =np.sum((ytrue_==1) & (ypred_==1))\n",
        "    TrueN =np.sum((ytrue_==0) & (ypred_==0))\n",
        "    FalseP =np.sum((ytrue_==0) & (ypred_==1))\n",
        "    FalseN =np.sum((ytrue_==1) & (ypred_==0))\n",
        "\n",
        "    #accuracy calculation\n",
        "    accuracy =(TrueP + TrueN) / (TrueP + TrueN + FalseP + FalseN)\n",
        "    #precision calculation\n",
        "    precision =TrueP / (TrueP + FalseP) if (TrueP + FalseP) else 0\n",
        "    #recall calculation\n",
        "    recall =TrueP / (TrueP + FalseN) if (TrueP + FalseN) else 0\n",
        "    #f1-score calculation\n",
        "    f1score =(2* precision* recall) / (precision + recall) if (precision + recall) else 0\n",
        "    #balanced accuracy calculation\n",
        "    TruePR = TrueP / (TrueP + FalseN)\n",
        "    TrueNR = TrueN / (TrueN + FalseP)\n",
        "    balanced_accuracy = 0.5 * (TruePR + TrueNR)\n",
        "\n",
        "    print(f\" Confusion matrix:\\n TP: {TrueP}, FP: {FalseP}\\n FN: {FalseN}, TN: {TrueN}\\n\")\n",
        "    print(f\" Accuracy:{accuracy:.4f}\")\n",
        "    print(f\" Balanced Accuracy:{balanced_accuracy:.4f}\\n\")\n",
        "    print(f\" Recall:{recall:.4f}\")\n",
        "    print(f\" Precision:{precision:.4f}\")\n",
        "    print(f\" f1 Score:{f1score:.4f}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "#performance measure\n",
        "evaluation(testy1,test1_predictions)\n",
        "\n",
        "#------------------predicting the labels for test set 2-------------\n",
        "#load the testset 2 data\n",
        "test_data2 =pd.read_csv('UNSWNB15_testing2_coursework_no_label.csv')\n",
        "\n",
        "#perform one hot encoding on test set 2\n",
        "test_data2 =pd.get_dummies(test_data2, columns=categorical_data)\n",
        "\n",
        "#align the columns to match the training data\n",
        "test_data2 =test_data2.reindex(columns=traindata.drop(columns=['label']).columns, fill_value=0)\n",
        "\n",
        "#standardization\n",
        "test2_x =test_data2.values.astype(np.float64)\n",
        "test2_x =(test2_x - mean_avg) / st_dev\n",
        "\n",
        "#predicting the labels for test set 2\n",
        "test2_predictions =(mlp.forward(test2_x) > 0.5).astype(int)\n",
        "\n",
        "print(\"Predictions for Test Set 2:\")\n",
        "print(test2_predictions.flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iCUL48FBwlQ"
      },
      "source": [
        "# ***4. Performance  Evaluation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePqjGPnHmYWv"
      },
      "source": [
        "Accuracy on Testing Set 1:  0.9050\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpMWHu9bCFSo"
      },
      "source": [
        "Predicted class labels for the data samples in the testing set 2 below:(0 or 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_8B_MxvCnf3"
      },
      "source": [
        "| **Sample ID** |**Predicted Label** |\n",
        "| --- | --- |\n",
        "| 1 | 0  |\n",
        "| 2 | 0  |\n",
        "| 3 | 1  |\n",
        "| 4 | 1  |\n",
        "| 5 | 0  |\n",
        "| 6 | 1  |\n",
        "| 7 | 0  |\n",
        "| 8 | 0  |\n",
        "| 9 |  1 |\n",
        "| 10 | 0  |\n",
        "| 11 |  0 |\n",
        "| 12 | 1 |\n",
        "| 13 |  1 |\n",
        "| 14 |  1|\n",
        "| 15 |  1 |\n",
        "| 16 | 0|\n",
        "| 17 |  1|\n",
        "| 18 |  1|\n",
        "| 19 |  0|\n",
        "| 20 |  1|\n",
        "| 21 |  1|\n",
        "| 22 |  1|\n",
        "| 23 | 0 |\n",
        "| 24 |  1 |\n",
        "| 25 | 1 |"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}